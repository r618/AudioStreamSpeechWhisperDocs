::o9.2o23::2022.2.0+
-------------------------------------------------------------------------------

    Asset is an Unity user friendly interface for - originally - OpenAI's Whisper speech recognition system -

    The easiest way to start is to run demo as mentioned in the '!!! README FIRST', or drop prefab from 
'AudioStreamSpeechWhisper\Prefabs' into scene.


- it is optimized to run on Apple Silicon by default, but Windows, Linux and Android binaries are still provided
(see also 'Platform specific native libraries' below)
- while on non Apple HW the CPU usage is rather high, small models run almost in realtime for quick/short speech
(see also for example AudioStreamSpeechWhisper_VoiceCommandDemo)
- large models demand large RAM to be available - see below

In general it's recommended to run either on Apple Silicon, or on desktop/standalone
and having at most one running instance of Whisper in a scene at a time

-------------------------------------------------------------------------------
Usage/Scripting:
-------------------------------------------------------------------------------

    After setting all/needed properties on the component, either set 'Auto Start', or call .StartRecognition() via scripting to run the component,
standard Unity events are used for all notifications afterwards.
(component has also 'StartSpeech' public method available, this is just for convenience and they are identical otherwise)

    - Whisper supports transcription, translation to English, and language detection operations and needs a model to run:
pretrained models are downloaded automatically (currently) from huggingface.co, a custom model can be specified as local file via 'customModelFilepath'
If the model is not found download is started and download progress is reported via 'modelDownloadProgress'/'modelDownloadBytes'
Note advanced models can be large, see 'Pretrained models details' below for details.

    - Local cache for downloaded models can be set by 'Download Cache Path' on configuration SO at
'AudioStreamSpeechWhisper\AudioStreamSpeech\Support\Resources\AudioStreamRuntimeSettings'
and by default is at Application.persistentDataPath

    - you can use prefab in 'AudioStreamSpeechWhisper\Prefabs'

Limitations:

    - please be advised that once a Whisper operation starts it's generally uncancellable - although C# component can be stopped,
Whisper will always run on its own separate thread until full completion of the last operation (this includes e.g. exiting Play mode)

    - a single instance in a scene is recommended ( at least with a running operation ) - multiple concurrent Whisper runs are currently not 
    supported (technically this is due to having static callbacks and might be fixed in the fututre, but due to performance demands it's currently not a priority)
   

-------------------------------------------------------------------------------
The AudioStreamSpeechWhisper component:
-------------------------------------------------------------------------------

Apart from Tooltips provided information on all public fields in the Inspector, more detailed info for some public properties:

Speech source:

- 'speechSource' set to:

    UNITY_MICROPHONE, 'microphoneDeviceId' specifies Id of microphone device to be used for speech recording as reported by Unity Microphone class,
        i.e. Microphone.devices[microphoneDeviceId] (see also https://docs.unity3d.com/ScriptReference/Microphone-devices.html)

    FILE, 'wavFileName' expects path+complete filename of an audio file accessible on local filesystem (file won't be donwloaded)
        - path needs only to be resolvable, i.e. can be relative - .NET FileInfo is used to access it
        - only WAV format is supported this way; if the file is not in Whisper supported format - MONO, 16 kHZ - it will be downmixed to MONO (might take some time if it's large - this will be updated in following releases)
        and resampled to 16 kHZ - so same performace limitations apply

Microphone processing:

- 'dB Threshold':
    - only audio above this dB value will be considered for processing
    - you can partially filter out surrounding noise while speaking closer / more clearly to the mic
    - if VAD processing is OFF this signal above dB threshold is sent to Whisper as is, when user script invokes 'ProcessSamples' - see 'AudioStreamSpeechWhisper_MicDemo'

- 'Use VAD': 
    - if ON, apart from db Threshold above, a Voice Activity Detection is applied to the signal which is split and sent to Whisper automatically in detected words / utterances,
    the silence gap between individual parts is regulated by next parameter, 'VAD Sensitivity Ms'
    - when ON this also causes Whisper to deliver all results in a single callback - this is useful also for short commands (but not necessary)
    / when OFF the recognized speech can be delivered in multiple callbacks/events

- 'VAD Sensitivity Ms':
    - this is approximated time in milliseconds which is considered to be still speech signal after the end of speech/utterance is detected by VAD,
    small values detect gaps/pauses in speech more often, large values allow e.g. for multiple words/sentences to be sent for Whisper processing at once

- 'microphoneProcessingState':
    alternates between Listening and Processing depending on whether Whisper is currently processing (previously recorded) microphone audio

- 'OnMicrophoneListening'/'OnMicrophoneProcessing':
    - used in the demo to display state, but also internally to alternate between listening and Whisper processing the speech

------------------
Whisper operations:
    
- 'operation':
    - just selects requested operation to perform
    - (default) transcription using set 'language', translation to English (from set 'language'), or detect language

- 'language':
    this is Whisper specific language code, as defined in original C++ implementation
        (see also e.g. https://github.com/ggerganov/whispr.cpp/blob/2f889132c66051b14c6f8770e9b3d4e3f159821d/whisper.cpp#L119)

- 'OnWhisperLanguageDetected'/'OnWhisperSegmentDetected':
    invoked as/when needed by Whisper


-------------------------------------------------------------------------------
Platform specific native libraries:
-------------------------------------------------------------------------------

- C# interface [https://github.com/r618/whisper.net.unitysubset] is tied to a specific Whisper/Whisper.NET version with platform native plugins included

Native Whisper libraries can be also built or downloaded from [https://github.com/sandrohanea/whisper.net], or built directly from [https://github.com/ggerganov/whisper.cpp]

iOS native plugin is built without bitcode - when building Unity Xcode project, please build UnityFramework without bitcode too 
(Unity Xcode projects still tend to have bitcode ON until it is completely deprecated in Xcode)

- Apple Silicon library included is built with CoreML support
    please note that normal (downloaded) models won't suffice though, and there are additional `mlmodelc` models files needed
    the library will fallback to original core library if `mlmodelc` is not found, for more please see
    https://github.com/sandrohanea/whisper.net#coreml-runtime
    [ once `mlmodelc` is created, a custom model local path can be specified on the component which will be used ]

- x64 Windows libraries can be used with GPU support -
    see https://github.com/sandrohanea/whisper.net#gpu-support-on-windows
    replace needed library in `Plugins\win\win-x64`


-------------------------------------------------------------------------------
Pretrained models details:
-------------------------------------------------------------------------------

    The original Whisper PyTorch models provided by OpenAI are converted to custom ggml format
for usage by whisper.cpp - for more details see [https://github.com/ggerganov/whisper.cpp/tree/master/models]

If selected (non custom) model is not found on local machine it will be automatically downloaded into local cache (see Usage/Scripting above),
with endpoints following [https://github.com/ggerganov/whisper.cpp/blob/master/models/download-ggml-model.sh]
(except SHA is not checked)

See also disk/RAM requirements for each model there 
- roughly 
    model   Disk    Mem
    tiny	75 MB	~390 MB
    base	142 MB  ~500 MB
    small	466 MB  ~1.0 GB
    medium	1.5 GB  ~2.6 GB
    large	2.9 GB  ~4.7 GB

Large/r models provide better quality and accuracy and might succeed where smaller fail in your specific scenario.
You can use a custom model if needed - in which case select CUSTOM and provide local filepath to it. The file won't be downloaded in that case.
