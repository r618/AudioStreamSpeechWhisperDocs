::o9.2o23::2022.2.0+
-------------------------------------------------------------------------------

    Asset is an Unity user friendly interface for - originally - OpenAI's Whisper speech recognition system -

    The easiest way to start is to run demo as mentioned in the '!!! README FIRST', or drop prefab from 
'AudioStreamSpeechWhisper\Prefabs' into scene.

- it is optimized to run on Apple Silicon by default, but Windows, Linux and Android binaries are still usable and are provided
(see also 'Platform specific native libraries' below)
- while on non Apple Silicon HW the CPU usage can be rather high, although small models run almost in realtime for quick/short speech
(see also for example AudioStreamSpeechWhisper_VoiceCommandDemo), on Windows/Linux it's also possible to use HW optimized Clblast/Cublas runtime/s
- large models demand large RAM to be available - see below

In general it's recommended to run either on Apple Silicon, or accelerated version/s on desktop/standalone
and having at most one running instance of Whisper in scene at a time

-------------------------------------------------------------------------------
Usage/Scripting:
-------------------------------------------------------------------------------

    After setting all/needed properties on the component, either set 'Auto Start', or call .StartRecognition() via scripting to run the component,
standard Unity events are used for all notifications afterwards.
(component has also 'StartSpeech' public method available, this is just for convenience and they are identical otherwise)

    - Whisper supports transcription, translation to English, and language detection operations and needs a model to run:
pretrained models are downloaded automatically (currently) from huggingface.co
If model is not found locally its download is started automatically (while in progress, 'modelDownloading' is set)

Note advanced models can be large, see 'Pretrained models details' below for details.

    - Local cache for downloaded models can be set by 'Download Cache Path' on configuration ScriptableObject at
'AudioStreamSpeechWhisper\AudioStreamSpeech\Support\Resources\AudioStreamRuntimeSettings'
and by default is at Application.persistentDataPath

    - you can use prefab in 'AudioStreamSpeechWhisper\Prefabs'

Limitations:

    - please be advised that once a Whisper operation starts it's generally uncancellable - although C# component can be stopped,
Whisper will always run on its own separate thread until full completion of the last operation (this includes e.g. exiting Play mode)

    - a single instance in a scene is recommended ( at least with a running operation ), though multiple concurrent Whisper runs are supported
   
-------------------------------------------------------------------------------
The AudioStreamSpeechWhisper component:
-------------------------------------------------------------------------------

Apart from Tooltips provided information on all public fields in the Inspector, more detailed info for some public properties:

Speech source:

- 'speechSource' set to:

    UNITY_MICROPHONE, 'microphoneDeviceId' specifies Id of microphone device to be used for speech recording as reported by Unity Microphone class,
        i.e. Microphone.devices[microphoneDeviceId] (see also https://docs.unity3d.com/ScriptReference/Microphone-devices.html)

    FILE, 'wavFileName' expects path+complete filename of an audio file accessible on local filesystem (file won't be donwloaded)
        - path needs only to be resolvable, i.e. can be relative - .NET FileInfo is used to access it
        - only WAV format is supported this way; if the file is not in Whisper supported format - MONO, 16 kHZ - it will be downmixed to MONO (might take some time if it's large - this will be updated in following releases)
        and resampled to 16 kHZ - so same performace limitations apply

Microphone processing:

- 'dB Threshold':
    - only audio above this dB value will be considered for processing
    - you can partially filter out surrounding noise while speaking closer / more clearly to the mic
    - if VAD processing is OFF this signal above dB threshold is sent to Whisper as is, when user script invokes 'ProcessSamples' - see 'AudioStreamSpeechWhisper_MicDemo'

- 'Use VAD': 
    - if ON, apart from db Threshold above, a Voice Activity Detection is applied to the signal which is split and sent to Whisper automatically in detected words / utterances,
    the silence gap between individual parts is regulated by next parameter, 'VAD Sensitivity Ms'
    - when ON this also causes Whisper to deliver all results in a single callback - this is useful also for short commands (but not necessary)
    / when OFF the recognized speech can be delivered in multiple callbacks/events

- 'VAD Sensitivity Ms':
    - this is approximated time in milliseconds which is considered to be still speech signal after the end of speech/utterance is detected by VAD,
    small values detect gaps/pauses in speech more often, large values allow e.g. for multiple words/sentences to be sent for Whisper processing at once

- 'microphoneProcessingState':
    alternates between Listening and Processing depending on whether Whisper is currently processing (previously recorded) microphone audio

- 'OnMicrophoneListening'/'OnMicrophoneProcessing':
    - used in the demo to display state, but also internally to alternate between listening and Whisper processing the speech

------------------
Whisper operations:
    
- 'operation':
    - just selects requested operation to perform
    - (default) transcription using set 'language', translation to English (from set 'language'), or detect language

- 'language':
    this is Whisper specific language code, as defined in original C++ implementation
        (see also e.g. https://github.com/ggerganov/whispr.cpp/blob/2f889132c66051b14c6f8770e9b3d4e3f159821d/whisper.cpp#L119)

- 'OnWhisperLanguageDetected'/'OnWhisperSegmentDetected':
    invoked as/when needed by Whisper


-------------------------------------------------------------------------------
Platform specific native libraries:
-------------------------------------------------------------------------------

- the asset includes native Whisper libraries for Windows, Linux and Android, macOS and iOS

- Windows, Linux and Android runtimes are used from [https://github.com/sandrohanea/whisper.net/tree/main/Whisper.net.Runtime]
    (only most commonly used platforms, non HW accelerated variants)
    You can use GPU HW accelerated versions - Clblast/Cublas on Windows/Linux - by replacing respective library in project in `Plugins\win` | `Plugins\linux`
    GPU support will be used automatically when library is loaded by Unity
    see also https://github.com/sandrohanea/whisper.net#gpu-support-on-windows

- iOS and macOS native libraries are built directly from whisper.cpp [https://github.com/ggerganov/whisper.cpp] with CoreML support
/ please be aware that iOS lib is built without bitcode, so in order to build Unity Xcode project, please turn off bitcode for UnityFramework
, or build (let me know) iOS lib with bitcode enabled, if you really need it.
(Unity Xcode projects still tend to have bitcode ON until it is completely deprecated in Xcode)

    on iOS and macOS a corresponding CoreML `mlmodelc` archive is also automatically downloaded and extracted and placed next to downloaded model file
    Whisper's CoreML support will autmoatically use it (if it's not able it will fallback to 'normal' CPU inference)

For other Apple platforms you most likely will need to build from whisper.cpp directly since native libraries currently present at
[https://github.com/sandrohanea/whisper.net/tree/main/Whisper.net.Runtime] are not suitable for static linking


- C# interface whisper.net.unitysubset [https://github.com/r618/whisper.net.unitysubset] is tied to a specific released Whisper/Whisper.NET version

Native whisper library log output is also printed to Unity Console (depending on Log Level set on the component; errors are always printed)


-------------------------------------------------------------------------------
Pretrained models details:
-------------------------------------------------------------------------------

    The original Whisper PyTorch models provided by OpenAI are converted to custom ggml format
for usage by whisper.cpp - for more details see [https://github.com/ggerganov/whisper.cpp/tree/master/models]

If selected model is not found on local machine it will be automatically downloaded into local cache (see Usage/Scripting above),
with endpoints following [https://huggingface.co/sandrohanea/whisper.net/]
(except SHA is not checked)

See also disk/RAM requirements for each model there 
- roughly 
    model   Disk    Mem
    tiny	75 MB	~390 MB
    base	142 MB  ~500 MB
    small	466 MB  ~1.0 GB
    medium	1.5 GB  ~2.6 GB
    large	2.9 GB  ~4.7 GB

Large/r models provide better quality and accuracy and might succeed where smaller fail in your specific scenario.
You can use a custom model if needed - in which case select CUSTOM and provide local filepath to it. The file won't be downloaded in that case.
